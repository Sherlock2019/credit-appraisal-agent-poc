# services/api/routers/training.py
from __future__ import annotations

import os
import glob
import json
import time
import uuid
import shutil
from datetime import datetime
from typing import List, Dict, Any, Optional

import pandas as pd
import numpy as np
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
import joblib

RUNS_DIR = os.path.expanduser("~/demo-library/services/api/.runs")
MODELS_ROOT = os.path.expanduser("~/demo-library/agents/credit_appraisal/models")
TRAINED_DIR = os.path.join(MODELS_ROOT, "trained")
PROD_DIR = os.path.join(MODELS_ROOT, "production")
os.makedirs(TRAINED_DIR, exist_ok=True)
os.makedirs(PROD_DIR, exist_ok=True)

router = APIRouter(prefix="/v1/training", tags=["training"])

# ─────────────────────────────────────────────
# Schemas

class FeedbackItem(BaseModel):
    application_id: str
    ai_decision: Optional[str] = None
    human_decision: Optional[str] = None
    human_rule_reasons: Optional[Any] = None

class FeedbackPayload(BaseModel):
    items: List[FeedbackItem]

class TrainRequest(BaseModel):
    base_csv_globs: List[str] = []
    cutoff_date: Optional[str] = None
    # naming fields
    model_name: str = "credit_lr"
    username: str = "unknown"
    agent_type: str = "credit_appraisal"

# ─────────────────────────────────────────────
# Utilities

def _read_many_csv(globs: List[str]) -> pd.DataFrame:
    frames: List[pd.DataFrame] = []
    for g in globs:
        for p in glob.glob(os.path.expanduser(g)):
            try:
                frames.append(pd.read_csv(p))
            except Exception:
                pass
    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()

def _coerce_label(df: pd.DataFrame,
                  human_col: str = "human_decision",
                  ai_col: str = "decision",
                  threshold_col: Optional[str] = None) -> np.ndarray:
    """
    Derive binary label: use human_decision if present, else fall back to AI decision.
    """
    if human_col in df.columns and df[human_col].notna().any():
        col = df[human_col].fillna("")
    elif ai_col in df.columns:
        col = df[ai_col].fillna("")
    else:
        # try threshold on probability column if provided
        if threshold_col and threshold_col in df.columns:
            return (df[threshold_col].astype(float) >= 0.5).astype(int).values
        raise ValueError("No label source found.")
    return (col.astype(str) == "approved").astype(int).values

def _safe_name(s: str) -> str:
    return "".join(ch for ch in s if ch.isalnum() or ch in ("-", "_")).strip() or "user"

def _now_stamp() -> str:
    return datetime.now().strftime("%Y%m%d-%H%M%S")

# ─────────────────────────────────────────────
# Endpoints

@router.post("/feedback")
def post_feedback(payload: FeedbackPayload):
    # Store a normalized CSV under .runs/reviewed/
    out_dir = os.path.join(RUNS_DIR, "reviewed")
    os.makedirs(out_dir, exist_ok=True)
    ts = _now_stamp()
    rows = [i.dict() for i in payload.items]
    df = pd.DataFrame(rows)
    out_path = os.path.join(out_dir, f"reviewed_{ts}.csv")
    df.to_csv(out_path, index=False)
    return {"status": "ok", "path": out_path, "rows": len(df)}

@router.post("/train")
def post_train(req: TrainRequest):
    # 1) load base data (e.g., AI outputs or historical)
    base_df = _read_many_csv(req.base_csv_globs)

    # 2) load latest human-reviewed CSVs if present
    reviewed_glob = os.path.join(RUNS_DIR, "reviewed", "*.csv")
    reviewed_df = _read_many_csv([reviewed_glob])

    if base_df.empty and reviewed_df.empty:
        raise HTTPException(status_code=400, detail="No training data found. Provide base_csv_globs or reviewed CSVs.")

    df = pd.concat([base_df, reviewed_df], ignore_index=True) if not base_df.empty else reviewed_df.copy()
    df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=1, how="all")

    # 3) features + labels
    # Simple numeric feature pick; in your real code, align with model_utils.FEATURES
    feature_cols = [c for c in df.columns if df[c].dtype.kind in "if" and c not in ("approved",)]
    if not feature_cols:
        raise HTTPException(status_code=400, detail="No numeric features available to train on.")
    X = df[feature_cols].fillna(0.0).values

    try:
        y = _coerce_label(df, human_col="human_decision", ai_col="decision", threshold_col=None)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Label derivation failed: {e}")

    # Must have both classes
    classes = np.unique(y)
    if len(classes) < 2:
        raise HTTPException(status_code=400, detail="Training requires at least two classes; provide mixed approvals/denials.")

    # 4) split & class weights
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    class_weights = compute_class_weight(class_weight="balanced", classes=np.array([0,1]), y=ytr)
    weights = {0: class_weights[0], 1: class_weights[1]}

    # 5) train a simple baseline
    clf = LogisticRegression(max_iter=400, class_weight=weights, n_jobs=None)
    clf.fit(Xtr, ytr)

    # 6) evaluate
    proba = clf.predict_proba(Xte)[:,1]
    pred = (proba >= 0.5).astype(int)
    metrics = {
        "auc": float(roc_auc_score(yte, proba)),
        "f1": float(f1_score(yte, pred)),
        "accuracy": float(accuracy_score(yte, pred)),
        "n_train": int(len(ytr)),
        "n_test": int(len(yte)),
    }

    # 7) persist candidate with your naming scheme
    user = _safe_name(req.username)
    mname = _safe_name(req.model_name)
    atype = _safe_name(req.agent_type)
    stamp = _now_stamp()
    fname = f"{mname}-{user}-{atype}-{stamp}.joblib"
    fjson = f"{mname}-{user}-{atype}-{stamp}.json"
    model_path = os.path.join(TRAINED_DIR, fname)
    meta_path  = os.path.join(TRAINED_DIR, fjson)

    joblib.dump(clf, model_path)
    meta = {
        "filename": fname,
        "model_path": model_path,
        "agent_type": atype,
        "model_name": mname,
        "username": user,
        "trained_at": stamp,
        "feature_cols": feature_cols,
        "metrics": metrics,
    }
    with open(meta_path, "w") as f:
        json.dump(meta, f, indent=2)

    # also write a pointer to "latest candidate"
    latest_dir = os.path.join(RUNS_DIR, "models")
    os.makedirs(latest_dir, exist_ok=True)
    with open(os.path.join(latest_dir, "last_candidate.json"), "w") as f:
        json.dump(meta, f, indent=2)

    return {"status": "ok", "job_id": stamp, "candidate": meta}

@router.post("/promote")
def post_promote():
    # read last candidate pointer
    last_meta_path = os.path.join(RUNS_DIR, "models", "last_candidate.json")
    if not os.path.exists(last_meta_path):
        raise HTTPException(status_code=400, detail="No candidate model found. Train first.")

    with open(last_meta_path, "r") as f:
        meta = json.load(f)

    src_model = meta["model_path"]
    if not os.path.exists(src_model):
        raise HTTPException(status_code=400, detail=f"Candidate file missing: {src_model}")

    # copy to production canonical name
    prod_model = os.path.join(PROD_DIR, "credit_model.joblib")
    shutil.copy2(src_model, prod_model)

    prod_meta = {
        "version": meta["trained_at"],
        "source": meta["filename"],
        "promoted_at": _now_stamp(),
        "model_name": meta.get("model_name"),
        "username": meta.get("username"),
        "agent_type": meta.get("agent_type"),
        "metrics": meta.get("metrics"),
        "feature_cols": meta.get("feature_cols"),
    }
    with open(os.path.join(PROD_DIR, "meta.json"), "w") as f:
        json.dump(prod_meta, f, indent=2)

    return {"status": "ok", "production": prod_meta}

@router.get("/production_meta")
def get_production_meta():
    mpath = os.path.join(PROD_DIR, "meta.json")
    if not os.path.exists(mpath):
        return {"has_production": False}
    with open(mpath, "r") as f:
        meta = json.load(f)
    meta["has_production"] = True
    return meta

