# services/ui/app.py
from __future__ import annotations

import os
import io
import re
import json
import time
import datetime as dt
from typing import Dict, Any, List, Optional, Tuple

import numpy as np
import pandas as pd
import requests
import streamlit as st
import altair as alt

# ─────────────────────────────────────────────
# App / API config
# ─────────────────────────────────────────────
API_URL = os.getenv("API_URL", "http://localhost:8090").rstrip("/")
RUNS_DIR = os.path.expanduser("~/demo-library/services/api/.runs")
os.makedirs(RUNS_DIR, exist_ok=True)

st.set_page_config(page_title="AI Credit Appraisal Platform", layout="wide")

# Altair theme — readable, consistent
alt.data_transformers.disable_max_rows()
def _alt_theme():
    return {
        "config": {
            "view": {"continuousWidth": 400, "continuousHeight": 300},
            "axis": {"labelFontSize": 12, "titleFontSize": 12},
            "legend": {"labelFontSize": 12, "titleFontSize": 12},
        }
    }
alt.themes.register("clean", _alt_theme)
alt.themes.enable("clean")

# ─────────────────────────────────────────────
# Header — user identity
# ─────────────────────────────────────────────
st.title("💳 AI Credit Appraisal Platform")
st.caption("Generate, sanitize, and appraise credit datasets securely — with tunable lending policies.")

with st.container():
    st.markdown("### 🧑 User Identity & Security Info")
    c1, c2, c3 = st.columns([1.5, 1.5, 1])
    with c1:
        user_name = st.text_input("Your Name (required)", value=st.session_state.get("ui_user_name", ""), placeholder="e.g. Alice Nguyen", key="ui_user_name")
    with c2:
        user_email = st.text_input("Email (required)", value=st.session_state.get("ui_user_email", ""), placeholder="e.g. alice@bank.com", key="ui_user_email")
    with c3:
        flag_session = st.checkbox("⚠️ Flag for Security Review", value=st.session_state.get("ui_flagged", False), key="ui_flagged")

    if not user_name or not user_email:
        st.warning("Please enter your name and email before proceeding.")
        st.stop()

st.session_state["user_info"] = {
    "name": user_name.strip(),
    "email": user_email.strip(),
    "flagged": bool(flag_session),
    "timestamp": dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
}

# ─────────────────────────────────────────────
# Globals / utils
# ─────────────────────────────────────────────
BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")

def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]

def scrub_text_pii(val):
    if not isinstance(val, str):
        return val
    val = EMAIL_RE.sub("", val)
    val = PHONE_RE.sub("", val)
    return val.strip()

def drop_pii_columns(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    original = list(df.columns)
    keep = [c for c in original if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original if c not in keep]
    out = df[keep].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped

def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in df.columns if c.lower() not in BANNED_NAMES]
    return df[keep]

def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)

def save_to_runs_csv(df: pd.DataFrame, prefix: str) -> str:
    ts = dt.datetime.now().strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath

def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None

# Currency helpers
CURRENCY_OPTIONS = {
    "USD": ("USD $", "$", 1.0),
    "EUR": ("EUR €", "€", 0.93),
    "GBP": ("GBP £", "£", 0.80),
    "JPY": ("JPY ¥", "¥", 150.0),
    "VND": ("VND ₫", "₫", 24000.0),
}
def set_currency_defaults():
    if "currency_code" not in st.session_state:
        st.session_state["currency_code"] = "USD"
    label, symbol, fx = CURRENCY_OPTIONS[st.session_state["currency_code"]]
    st.session_state["currency_label"] = label
    st.session_state["currency_symbol"] = symbol
    st.session_state["currency_fx"] = fx
def fmt_currency_label(base: str) -> str:
    sym = st.session_state.get("currency_symbol", "")
    return f"{base} ({sym})" if sym else base
set_currency_defaults()

# Backend helpers
def api_get(path: str, **kwargs):
    try:
        r = requests.get(f"{API_URL}{path}", timeout=kwargs.pop("timeout", 10), **kwargs)
        return r
    except Exception as e:
        return None

def api_post(path: str, **kwargs):
    try:
        r = requests.post(f"{API_URL}{path}", timeout=kwargs.pop("timeout", 30), **kwargs)
        return r
    except Exception:
        return None

# ─────────────────────────────────────────────
# Tabs
# ─────────────────────────────────────────────
tab_gen, tab_clean, tab_run, tab_review, tab_train = st.tabs([
    "🏦 Synthetic Data Generator",
    "🧹 Anonymize & Sanitize Data",
    "🤖 Credit appraisal by AI assistant",
    "🧑‍⚖️ Human Review",
    "🔁 Training (Feedback → Retrain)",
])

# ─────────────────────────────────────────────
# Data generation
# ─────────────────────────────────────────────
def _gen_base(n: int, non_bank_ratio: float, pii: bool) -> pd.DataFrame:
    rng = np.random.default_rng(42)

    # customer type
    is_non = rng.random(n) < non_bank_ratio
    cust_type = np.where(is_non, "non-bank", "bank")

    data = {
        "application_id": [f"APP_{i:05d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["real_estate", "car", "land", "deposit"], n),
        "co_loaners": rng.choice([0, 1, 2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
        "current_loans": rng.integers(0, 5, n),
        "customer_type": cust_type,
    }
    if pii:
        names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran","Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
        emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
        addrs = [
            "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
            "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
            "8 Rue Lafayette, Paris, FR","21 Königstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
        ]
        data.update({
            "customer_name": np.random.choice(names, n),
            "email": np.random.choice(emails, n),
            "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
            "address": np.random.choice(addrs, n),
            "national_id": rng.integers(10_000_000, 99_999_999, n),
        })

    df = pd.DataFrame(data)
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))

    # currency scaling
    fx = st.session_state["currency_fx"]
    for col in ["income", "loan_amount", "collateral_value", "assets_owned", "existing_debt"]:
        df[col] = (df[col] * fx).round(2)
    df["currency_code"] = st.session_state["currency_code"]

    return dedupe_columns(df)

def generate_raw_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    return _gen_base(n, non_bank_ratio, pii=True)

def generate_anon_synthetic(n: int, non_bank_ratio: float) -> pd.DataFrame:
    return _gen_base(n, non_bank_ratio, pii=False)

def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)

# ─────────────────────────────────────────────
# TAB 1 — Synthetic Data Generator
# ─────────────────────────────────────────────
with tab_gen:
    st.subheader("🏦 Synthetic Credit Data Generator")

    c1, c2 = st.columns([1, 2])
    with c1:
        code = st.selectbox(
            "Currency",
            list(CURRENCY_OPTIONS.keys()),
            index=list(CURRENCY_OPTIONS.keys()).index(st.session_state["currency_code"]),
            help="All monetary fields will be in this local currency.",
            key="currency_selector",
        )
        if code != st.session_state["currency_code"]:
            st.session_state["currency_code"] = code
            set_currency_defaults()
    with c2:
        st.info(f"Amounts will be generated in **{st.session_state['currency_label']}**.", icon="💰")

    rows = st.slider("Number of rows to generate", 50, 5000, 500, step=50)
    non_bank_ratio = st.slider("Share of non-bank customers", 0.0, 1.0, 0.30, 0.05)

    cA, cB = st.columns(2)
    with cA:
        if st.button("🔴 Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows, non_bank_ratio))
            st.session_state["synthetic_raw_df"] = raw_df
            path = save_to_runs_csv(raw_df, "synthetic_raw")
            st.success(f"Generated RAW dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button("⬇️ Download RAW CSV", raw_df.to_csv(index=False).encode("utf-8"), os.path.basename(path), "text/csv")

    with cB:
        if st.button("🟢 Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows, non_bank_ratio))
            st.session_state["synthetic_df"] = anon_df
            path = save_to_runs_csv(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows in {st.session_state['currency_label']}. Saved to {path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button("⬇️ Download ANON CSV", anon_df.to_csv(index=False).encode("utf-8"), os.path.basename(path), "text/csv")

# ─────────────────────────────────────────────
# TAB 2 — Anonymize & Sanitize Data
# ─────────────────────────────────────────────
with tab_clean:
    st.subheader("🧹 Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"], key="clean_uploader")
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("📊 Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state["anonymized_df"] = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("✅ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs_csv(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button("⬇️ Download Clean Data", sanitized.to_csv(index=False).encode("utf-8"), os.path.basename(fpath), "text/csv")
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="ℹ️")

# ─────────────────────────────────────────────
# TAB 3 — Credit appraisal by AI assistant
# ─────────────────────────────────────────────
with tab_run:
    st.subheader("🤖 Credit appraisal by AI assistant")

    # Production meta info (optional)
    prod = api_get("/v1/training/production_meta")
    if prod and prod.ok:
        meta = prod.json()
        if meta.get("has_production"):
            ver = (meta.get("meta") or {}).get("version", "1.x")
            src = (meta.get("meta") or {}).get("source", "production")
            st.success(f"🟢 Production model active — version: {ver} • source: {src}")
        else:
            st.warning("⚠️ No production model promoted yet — the agent will use baseline or latest trained.")
    else:
        st.info("ℹ️ Could not fetch production model meta.")

    # Model registry (trained list)
    trained_list: List[str] = []
    rlist = api_get("/v1/training/models?kind=trained")
    if rlist and rlist.ok:
        try:
            trained_list = rlist.json().get("models", [])
        except Exception:
            trained_list = []
    trained_list = sorted(trained_list)

    # Default selection: latest trained if available, else “(auto / production / baseline)”
    model_choices = ["(auto: latest-trained → production → baseline)"] + trained_list
    model_choice = st.selectbox("Scoring model", model_choices, index=0,
                                help="Pick a specific trained model, or let the backend select. Default uses your latest trained model if any.")

    # LLM + host hints (passed to API; optional)
    LLM_MODELS = [
        ("Phi-3 Mini (3.8B) — CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
        ("Mistral 7B Instruct — CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU ≥8GB"),
        ("Gemma-2 7B — CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU ≥8GB"),
        ("LLaMA-3 8B — GPU recommended", "llama3:8b-instruct", "GPU ≥12GB (CPU very slow)"),
        ("Qwen2 7B — GPU recommended", "qwen2:7b-instruct", "GPU ≥12GB (CPU very slow)"),
    ]
    LLM_LABELS = [l for (l, _, _) in LLM_MODELS]
    LLM_VALUE = {l:v for (l, v, _) in LLM_MODELS}
    LLM_HINT = {l:h for (l, _, h) in LLM_MODELS}

    FLAVORS = {
        "m4.medium":  "4 vCPU / 8 GB RAM — CPU-only small",
        "m8.large":   "8 vCPU / 16 GB RAM — CPU-only medium",
        "g1.a10.1":   "8 vCPU / 32 GB RAM + 1×A10 24GB",
        "g1.l40.1":   "16 vCPU / 64 GB RAM + 1×L40 48GB",
        "g2.a100.1":  "24 vCPU / 128 GB RAM + 1×A100 80GB",
    }

    with st.expander("🧠 Local LLM & Hardware Profile (optional hints)", expanded=False):
        colm, colh = st.columns([1.2, 1])
        with colm:
            model_label = st.selectbox("Local LLM (used for narratives/explanations)", LLM_LABELS, index=1)
            st.caption(f"Hint: {LLM_HINT[model_label]}")
        with colh:
            flavor = st.selectbox("OpenStack flavor / host profile", list(FLAVORS.keys()), index=0)
            st.caption(FLAVORS[flavor])
        use_llm = st.checkbox("Use LLM narrative", value=False)

    # Data source
    data_choice = st.selectbox(
        "Select Data Source",
        ["Use synthetic (ANON)", "Use synthetic (RAW – auto-sanitize)", "Use anonymized dataset", "Upload manually"],
        key="run_data_choice",
    )
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_run_uploader")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # Rules (Classic vs NDI)
    st.markdown("### ⚙️ Decision Rule Set")
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) — simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds."
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45, "min_emp_years": 2, "min_credit_hist": 3, "salary_floor": 3000,
        "max_delinquencies": 2, "max_current_loans": 3, "req_min": 1000, "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60], "threshold": 0.45, "target_rate": None, "random_band": True,
        "min_income_debt_ratio": 0.35, "compounded_debt_factor": 1.0, "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state["classic_rules"] = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state["ndi_rules"] = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state["classic_rules"] = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state["ndi_rules"] = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset)", expanded=True):
            rc = st.session_state["classic_rules"]
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, rc["max_dti"], 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, rc["min_emp_years"])
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, rc["min_credit_hist"])
            with r2:
                rc["salary_floor"] = st.number_input(fmt_currency_label("Minimum Monthly Salary"), 0, 10_000_000_000, rc["salary_floor"], step=1000)
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, rc["max_delinquencies"])
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, rc["max_current_loans"])
            with r3:
                rc["req_min"] = st.number_input(fmt_currency_label("Requested Amount Min"), 0, 10_000_000_000, rc["req_min"], step=1000)
                rc["req_max"] = st.number_input(fmt_currency_label("Requested Amount Max"), 0, 10_000_000_000, rc["req_max"], step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12, 24, 36, 48, 60, 72], default=rc["loan_terms"])
            st.markdown("#### 🧮 Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider("Min Income / (Compounded Debt) Ratio", 0.10, 2.00, rc["min_income_debt_ratio"], 0.01)
            with d2:
                rc["compounded_debt_factor"] = st.slider("Compounded Debt Factor (× requested)", 0.5, 3.0, rc["compounded_debt_factor"], 0.1)
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, rc["monthly_debt_relief"], 0.05)

            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                use_target = st.toggle("🎯 Use target approval rate", value=(rc["target_rate"] is not None))
            with c2:
                rc["random_band"] = st.toggle("🎲 Randomize approval band (20–60%) when no target", value=rc["random_band"])
            with c3:
                if st.button("↩️ Reset to defaults", key="reset_classic"):
                    reset_classic()
                    st.rerun()
            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rc["target_rate"] or 0.40, 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rc["threshold"], 0.01)
                rc["target_rate"] = None
    else:
        with st.expander("NDI Metrics (with Reset)", expanded=True):
            rn = st.session_state["ndi_rules"]
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input(fmt_currency_label("Min NDI (Net Disposable Income) per month"), 0.0, 1e12, float(rn["ndi_value"]), step=50.0)
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(rn["ndi_ratio"]), 0.01)
            c1, c2, c3 = st.columns([1, 1, 1])
            with c1:
                use_target = st.toggle("🎯 Use target approval rate", value=(rn["target_rate"] is not None))
            with c2:
                rn["random_band"] = st.toggle("🎲 Randomize approval band (20–60%) when no target", value=rn["random_band"])
            with c3:
                if st.button("↩️ Reset to defaults (NDI)", key="reset_ndi"):
                    reset_ndi()
                    st.rerun()
            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, rn["target_rate"] or 0.40, 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, rn["threshold"], 0.01)
                rn["target_rate"] = None

    # Run agent
    if st.button("🚀 Run Agent", use_container_width=True, key="run_agent_btn"):
        try:
            files = None
            # Prepare form fields
            form: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm if "use_llm" in locals() else False).lower(),
                "llm_model": LLM_VALUE[model_label] if "model_label" in locals() else "",
                "hardware_flavor": flavor if "flavor" in locals() else "",
                "currency_code": st.session_state["currency_code"],
                "currency_symbol": st.session_state["currency_symbol"],
                "selected_model": "" if model_choice.startswith("(auto") else model_choice,
            }
            if rule_mode.startswith("Classic"):
                rc = st.session_state["classic_rules"]
                form.update({
                    "rule_mode": "classic",
                    "min_employment_years": str(rc["min_emp_years"]),
                    "max_debt_to_income": str(rc["max_dti"]),
                    "min_credit_history_length": str(rc["min_credit_hist"]),
                    "max_num_delinquencies": str(rc["max_delinquencies"]),
                    "max_current_loans": str(rc["max_current_loans"]),
                    "requested_amount_min": str(rc["req_min"]),
                    "requested_amount_max": str(rc["req_max"]),
                    "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                    "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                    "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                    "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                    "salary_floor": str(rc["salary_floor"]),
                    "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                    "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                    "random_band": str(rc["random_band"]).lower(),
                    "random_approval_band": str(rc["random_band"]).lower(),
                })
            else:
                rn = st.session_state["ndi_rules"]
                form.update({
                    "rule_mode": "ndi",
                    "ndi_value": str(rn["ndi_value"]),
                    "ndi_ratio": str(rn["ndi_ratio"]),
                    "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                    "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                    "random_band": str(rn["random_band"]).lower(),
                    "random_approval_band": str(rn["random_band"]).lower(),
                })

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state["synthetic_df"], "synthetic_anon.csv")
            elif data_choice == "Use synthetic (RAW – auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state["synthetic_raw_df"], "synthetic_raw_sanitized.csv")
            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab."); st.stop()
                files = prep_and_pack(st.session_state["anonymized_df"], "anonymized.csv")
            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first."); st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection."); st.stop()

            r = requests.post(f"{API_URL}/v1/agents/credit_appraisal/run", data=form, files=files, timeout=240)
            if not r.ok:
                st.error(f"Run failed ({r.status_code}): {r.text}")
                st.stop()

            res = r.json()
            rid = res.get("run_id")
            st.session_state["last_run_id"] = rid
            st.success(f"✅ Run succeeded! Run ID: {rid}")

            # Pull merged.csv for dashboards/review
            merged_bytes = requests.get(f"{API_URL}/v1/runs/{rid}/report?format=csv", timeout=30).content
            merged_df = pd.read_csv(io.BytesIO(merged_bytes))
            st.session_state["last_merged_df"] = merged_df

            # Export AI outputs as CSV
            ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
            out_name = f"ai-appraisal-outputs-{ts}-{st.session_state['currency_code']}.csv"
            st.download_button("⬇️ Download AI outputs (CSV)", merged_df.to_csv(index=False).encode("utf-8"), out_name, "text/csv")

            # Decision filter IN the table
            st.markdown("#### Results (filter by decision within the grid)")
            df_view = merged_df.copy()
            uniq_dec = sorted(df_view.get("decision", pd.Series(dtype=str)).dropna().unique().tolist())
            chosen = st.multiselect("Decision", options=uniq_dec, default=uniq_dec, key="filter_decisions")
            if "decision" in df_view.columns and chosen:
                df_view = df_view[df_view["decision"].isin(chosen)]

            # Per-row metrics met / unmet
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k, v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")
            cols_show = [c for c in [
                "application_id","customer_type","decision","score","loan_amount","income",
                "metrics_met","metrics_unmet","proposed_loan_option","proposed_consolidation_loan","top_feature","explanation"
            ] if c in df_view.columns]
            st.dataframe(df_view[cols_show], use_container_width=True, height=400)

            # Customer mix summary (bank vs non-bank)
            st.markdown("#### Customer Mix")
            if "customer_type" in merged_df.columns:
                mix = merged_df["customer_type"].value_counts().rename_axis("type").reset_index(name="count")
                total = int(mix["count"].sum()) if not mix.empty else 0
                if total > 0:
                    mix["ratio"] = (mix["count"] / total).round(3)
                st.dataframe(mix, use_container_width=True)

            # Dashboard — pie + bars (pleasant color presets via Altair default)
            st.markdown("### 📊 Dashboard")

            # 1) Approval vs Denial — PIE
            if "decision" in merged_df.columns:
                pie_df = merged_df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
                chart1 = alt.Chart(pie_df).mark_arc(outerRadius=120, innerRadius=40).encode(
                    theta=alt.Theta("Count:Q"),
                    color=alt.Color("Decision:N", legend=alt.Legend(title=None)),
                    tooltip=["Decision", "Count"]
                ).properties(height=320, title="Approval rate")
                st.altair_chart(chart1, use_container_width=True)

            # 2) Avg DTI/LTV by decision — grouped bars
            if set(["decision","DTI","LTV"]).issubset(merged_df.columns):
                grp = merged_df.groupby("decision").agg(avg_DTI=("DTI","mean"), avg_LTV=("LTV","mean")).reset_index()
                long = grp.melt(id_vars=["decision"], value_vars=["avg_DTI","avg_LTV"], var_name="Metric", value_name="Value")
                chart2 = alt.Chart(long).mark_bar().encode(
                    x=alt.X("decision:N", title=None),
                    y=alt.Y("Value:Q"),
                    color=alt.Color("Metric:N"),
                    column=alt.Column("Metric:N", header=alt.Header(title=None)),
                    tooltip=["decision","Metric","Value"]
                ).properties(height=260, title="Risk thresholds (avg)")
                st.altair_chart(chart2, use_container_width=True)

            # 3) Top approved loan amounts — bars
            if set(["decision","loan_amount","application_id"]).issubset(merged_df.columns):
                acc = merged_df[merged_df["decision"] == "approved"].copy()
                if not acc.empty:
                    top_acc = acc.sort_values("loan_amount", ascending=False).head(15)
                    chart3 = alt.Chart(top_acc).mark_bar().encode(
                        x=alt.X("application_id:N", sort="-y", title="Application"),
                        y=alt.Y("loan_amount:Q", title=fmt_currency_label("Loan Amount")),
                        tooltip=["application_id","loan_amount"]
                    ).properties(height=300, title="Top approved loan amounts")
                    st.altair_chart(chart3, use_container_width=True)

            # 4) Collateral profile — bars
            if set(["collateral_type","collateral_value"]).issubset(merged_df.columns):
                cprof = merged_df.groupby("collateral_type").agg(avg_col=("collateral_value","mean"), cnt=("collateral_type","count")).reset_index()
                chart4 = alt.Chart(cprof).mark_bar().encode(
                    x=alt.X("collateral_type:N", sort="-y", title="Collateral Type"),
                    y=alt.Y("avg_col:Q", title=fmt_currency_label("Avg collateral value")),
                    tooltip=["collateral_type","avg_col","cnt"]
                ).properties(height=260, title="Collateral profile")
                st.altair_chart(chart4, use_container_width=True)

            # 5) Top proposed loan plans — horizontal bars
            if "proposed_loan_option" in merged_df.columns:
                plans = merged_df["proposed_loan_option"].dropna().astype(str)
                if len(plans) > 0:
                    top_plans = plans.value_counts().head(10).rename_axis("Plan").reset_index(name="Count")
                    chart5 = alt.Chart(top_plans).mark_bar().encode(
                        x=alt.X("Count:Q"),
                        y=alt.Y("Plan:N", sort="-x"),
                        tooltip=["Plan","Count"]
                    ).properties(height=320, title="Top proposed loan plans")
                    st.altair_chart(chart5, use_container_width=True)

            # Artifact links
            cdl1, cdl2, cdl3, cdl4, cdl5 = st.columns(5)
            with cdl1: st.markdown(f"[⬇️ PDF report]({API_URL}/v1/runs/{rid}/report?format=pdf)")
            with cdl2: st.markdown(f"[⬇️ Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
            with cdl3: st.markdown(f"[⬇️ Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
            with cdl4: st.markdown(f"[⬇️ Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
            with cdl5: st.markdown(f"[⬇️ JSON]({API_URL}/v1/runs/{rid}/report?format=json)")

        except Exception as e:
            st.exception(e)

    # Quick re-download
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("📥 Download Latest Outputs")
        rid = st.session_state["last_run_id"]
        c1, c2, c3, c4, c5 = st.columns(5)
        with c1: st.markdown(f"[⬇️ PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with c2: st.markdown(f"[⬇️ Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with c3: st.markdown(f"[⬇️ Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with c4: st.markdown(f"[⬇️ Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with c5: st.markdown(f"[⬇️ JSON]({API_URL}/v1/runs/{rid}/report?format=json)")

# ─────────────────────────────────────────────
# TAB 4 — Human Review
# ─────────────────────────────────────────────
with tab_review:
    st.subheader("🧑‍⚖️ Human Review — Correct AI Decisions & Score Agreement")

    # Load AI outputs for review (optional)
    uploaded_review = st.file_uploader("Load AI outputs CSV for review (optional)", type=["csv"], key="review_loader")
    if uploaded_review is not None:
        try:
            st.session_state["last_merged_df"] = pd.read_csv(uploaded_review)
            st.success("Loaded review dataset from uploaded CSV.")
        except Exception as e:
            st.error(f"Could not read uploaded CSV: {e}")

    if "last_merged_df" not in st.session_state:
        st.info("Run the agent (previous tab) or upload an AI outputs CSV to load results for review.")
    else:
        dfm = st.session_state["last_merged_df"].copy()

        st.markdown("#### 1) Select rows to review and correct")
        editable_cols = []
        if "decision" in dfm.columns: editable_cols.append("decision")
        if "rule_reasons" in dfm.columns: editable_cols.append("rule_reasons")
        if "customer_type" in dfm.columns: editable_cols.append("customer_type")

        editable = dfm[["application_id"] + editable_cols].copy()
        editable.rename(columns={"decision": "ai_decision"}, inplace=True)
        editable["human_decision"] = editable.get("ai_decision", "approved")
        editable["human_rule_reasons"] = editable.get("rule_reasons", "")

        edited = st.data_editor(
            editable,
            num_rows="dynamic",
            use_container_width=True,
            key="review_editor_grid",
            column_config={
                "human_decision": st.column_config.SelectboxColumn(options=["approved", "denied"]),
                "customer_type": st.column_config.SelectboxColumn(options=["bank","non-bank"], disabled=True)
            }
        )

        st.markdown("#### 2) Compute agreement score")
        if st.button("Compute agreement score", key="compute_agree"):
            if "ai_decision" in edited.columns and "human_decision" in edited.columns:
                agree = (edited["ai_decision"] == edited["human_decision"]).astype(int)
                score = float(agree.mean()) if len(agree) else 0.0
                st.success(f"Agreement score (AI vs human): {score:.3f}")
                st.session_state["last_agreement_score"] = score
            else:
                st.warning("Missing decision columns to compute score.")

        st.markdown("#### 3) Export feedback CSV (use in Training tab)")
        ts = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
        fb_name = f"feedback-{user_name}-{ts}.csv"
        st.download_button(
            "⬇️ Download feedback CSV",
            edited.to_csv(index=False).encode("utf-8"),
            fb_name,
            "text/csv",
            key="dl_feedback_csv",
        )

        st.markdown("#### 4) Submit feedback to API (optional)")
        if st.button("Submit feedback", key="submit_feedback_btn"):
            try:
                items = []
                for _, row in edited.iterrows():
                    items.append({
                        "application_id": row["application_id"],
                        "ai_decision": row.get("ai_decision"),
                        "human_decision": row.get("human_decision"),
                        "human_rule_reasons": row.get("human_rule_reasons"),
                    })
                r = api_post("/v1/training/feedback", json={"items": items}, timeout=60)
                if r and r.ok:
                    st.success(r.json())
                else:
                    st.error(r.text if r is not None else "Request failed.")
            except Exception as e:
                st.error(f"Feedback failed: {e}")

# ─────────────────────────────────────────────
# TAB 5 — Training (Feedback → Retrain)
# ─────────────────────────────────────────────
with tab_train:
    st.subheader("🔁 Human Feedback → Retrain")

    st.markdown("Upload your **feedback CSV** (from Human Review) then start training. After success, promote the model to production or keep using the **latest trained** by default in the agent panel.")

    # Stage feedback(s)
    staged_dir = os.path.join(RUNS_DIR, "tmp_feedback")
    os.makedirs(staged_dir, exist_ok=True)
    ups = st.file_uploader("Upload feedback CSV(s)", type=["csv"], accept_multiple_files=True, key="train_feedback_uploader")
    staged_paths: List[str] = []
    if ups:
        for u in ups:
            try:
                b = u.getvalue()
                outp = os.path.join(staged_dir, u.name)
                with open(outp, "wb") as f:
                    f.write(b)
                staged_paths.append(outp)
            except Exception:
                pass
        if staged_paths:
            st.success(f"Staged {len(staged_paths)} feedback file(s) to {staged_dir}")

    # Train request payload (manual loop)
    default_algo = "credit_lr"
    train_payload = {
        "feedback_csvs": staged_paths,
        "user_name": user_name,
        "agent_name": "credit_appraisal",
        "algo_name": default_algo,
    }
    st.markdown("#### Launch Retrain")
    st.code(json.dumps(train_payload, indent=2), language="json")

    cA, cB = st.columns([1, 1])
    with cA:
        if st.button("🚀 Train candidate model", key="train_btn"):
            r = api_post("/v1/training/train", json=train_payload, timeout=120)
            if r and r.ok:
                info = r.json()
                st.success(info)
                st.session_state["last_trained_model_name"] = info.get("model_name")
            else:
                st.error(r.text if r is not None else "Request failed.")
    with cB:
        model_to_promote = st.text_input("Model name to promote (from trained)", value=st.session_state.get("last_trained_model_name", ""))
        if st.button("⬆️ Promote to PRODUCTION", key="promote_btn"):
            r = api_post("/v1/training/promote", json={"model_name": model_to_promote}, timeout=60)
            if r and r.ok:
                st.success(r.json())
            else:
                st.error(r.text if r is not None else "Request failed.")

    st.markdown("---")
    st.markdown("#### Production Model")
    r = api_get("/v1/training/production_meta")
    if r and r.ok:
        st.json(r.json())
    else:
        st.info("No production model yet or endpoint unavailable.")
