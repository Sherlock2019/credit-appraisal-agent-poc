# services/ui/app.py
import os
import io
import re
import json
import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
from glob import glob

import pandas as pd
import numpy as np
import requests
import streamlit as st
import altair as alt

# ─────────────────────────────────────────────
# CONFIG
API_URL = os.getenv("API_URL", "http://localhost:8090")
RUNS_DIR = os.path.expanduser("~/demo-library/services/api/.runs")
os.makedirs(RUNS_DIR, exist_ok=True)

# Persisted user metric profiles
USER_METRICS_PATH = Path.home() / ".demo-library/user_metrics/metrics_profile.json"
USER_METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)

st.set_page_config(page_title="Credit Appraisal by AI Assistant", layout="wide")

# ─────────────────────────────────────────────
# HEADER — USER INFO + SECURITY
st.title("💳 Credit Appraisal by AI Assistant")
st.caption("Generate, sanitize, and appraise credit datasets securely — with tunable lending policies.")

with st.container():
    st.markdown("### 🧑 User Identity & Security Info")
    col1, col2, col3 = st.columns([1.5, 1.5, 1])
    with col1:
        user_name = st.text_input("Your Name (required)", value="", placeholder="e.g. Alice Nguyen")
    with col2:
        user_email = st.text_input("Email (required)", value="", placeholder="e.g. alice@bank.com")
    with col3:
        flag_session = st.checkbox("⚠️ Flag for Security Review", value=False)

    if not user_name or not user_email:
        st.warning("Please enter your name and email before proceeding.")
        st.stop()

st.session_state["user_info"] = {
    "name": user_name.strip(),
    "email": user_email.strip(),
    "flagged": flag_session,
    "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
}

# ─────────────────────────────────────────────
# TABS
tab_gen, tab_clean, tab_run, tab_review, tab_train = st.tabs([
    "🏦 Synthetic Data Generator",
    "🧹 Anonymize & Sanitize Data",
    "🤖 Credit Appraisal by AI Assistant",
    "🧑‍⚖️ Human Review",
    "🔁 Training (Feedback → Retrain)",
])

# ─────────────────────────────────────────────
# UTILITIES

BANNED_NAMES = {"race", "gender", "religion", "ethnicity", "ssn", "national_id"}
PII_COLS = {"customer_name", "name", "email", "phone", "address", "ssn", "national_id", "dob"}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_RE = re.compile(r"\+?\d[\d\-\s]{6,}\d")

def dedupe_columns(df: pd.DataFrame) -> pd.DataFrame:
    return df.loc[:, ~df.columns.duplicated(keep="last")]

def scrub_text_pii(s):
    if not isinstance(s, str):
        return s
    s = EMAIL_RE.sub("", s)
    s = PHONE_RE.sub("", s)
    return s.strip()

def drop_pii_columns(df: pd.DataFrame):
    original_cols = list(df.columns)
    keep_cols = [c for c in original_cols if all(k not in c.lower() for k in PII_COLS)]
    dropped = [c for c in original_cols if c not in keep_cols]
    out = df[keep_cols].copy()
    for c in out.select_dtypes(include="object"):
        out[c] = out[c].apply(scrub_text_pii)
    return dedupe_columns(out), dropped

def strip_policy_banned(df: pd.DataFrame) -> pd.DataFrame:
    keep = []
    for c in df.columns:
        cl = c.lower()
        if cl in BANNED_NAMES:
            continue
        keep.append(c)
    return df[keep]

def append_user_info(df: pd.DataFrame) -> pd.DataFrame:
    meta = st.session_state["user_info"]
    out = df.copy()
    out["session_user_name"] = meta["name"]
    out["session_user_email"] = meta["email"]
    out["session_flagged"] = meta["flagged"]
    out["created_at"] = meta["timestamp"]
    return dedupe_columns(out)

def save_to_runs(df: pd.DataFrame, prefix: str) -> str:
    ts = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M")
    flag_suffix = "_FLAGGED" if st.session_state["user_info"]["flagged"] else ""
    fname = f"{prefix}_{ts}{flag_suffix}.csv"
    fpath = os.path.join(RUNS_DIR, fname)
    dedupe_columns(df).to_csv(fpath, index=False)
    return fpath

def to_agent_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    Harmonize to the server-side agent’s expected schema.
    """
    out = df.copy()
    n = len(out)
    if "employment_years" not in out.columns:
        out["employment_years"] = out.get("employment_length", 0)
    if "debt_to_income" not in out.columns:
        if "DTI" in out.columns:
            out["debt_to_income"] = out["DTI"].astype(float)
        elif "existing_debt" in out.columns and "income" in out.columns:
            denom = out["income"].replace(0, np.nan)
            dti = (out["existing_debt"] / denom).fillna(0.0)
            out["debt_to_income"] = dti.clip(0, 10)
        else:
            out["debt_to_income"] = 0.0
    rng = np.random.default_rng(12345)
    if "credit_history_length" not in out.columns:
        out["credit_history_length"] = rng.integers(0, 30, n)
    if "num_delinquencies" not in out.columns:
        out["num_delinquencies"] = np.minimum(rng.poisson(0.2, n), 10)
    if "current_loans" not in out.columns:
        out["current_loans"] = rng.integers(0, 5, n)
    if "requested_amount" not in out.columns:
        out["requested_amount"] = out.get("loan_amount", 0)
    if "loan_term_months" not in out.columns:
        out["loan_term_months"] = out.get("loan_duration_months", 0)
    return dedupe_columns(out)

def try_json(x):
    if isinstance(x, (dict, list)):
        return x
    if not isinstance(x, str):
        return None
    try:
        return json.loads(x)
    except Exception:
        return None

def load_metrics_profile() -> Dict[str, Any]:
    try:
        if USER_METRICS_PATH.exists():
            return json.load(open(USER_METRICS_PATH))
    except Exception:
        pass
    return {}

def save_metrics_profile(metrics: Dict[str, Any]) -> None:
    try:
        with open(USER_METRICS_PATH, "w") as f:
            json.dump(metrics, f, indent=2)
    except Exception:
        pass

def agreement_score(ai_decisions: pd.Series, human_decisions: pd.Series) -> float:
    try:
        m = (ai_decisions.astype(str).str.lower() == human_decisions.astype(str).str.lower())
        return float(round(100.0 * m.mean(), 2)) if len(m) else 0.0
    except Exception:
        return 0.0

# ─────────────────────────────────────────────
# DATA GENERATORS
def generate_raw_synthetic(n: int) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    names = ["Alice Nguyen","Bao Tran","Chris Do","Duy Le","Emma Tran",
             "Felix Nguyen","Giang Ho","Hanh Vo","Ivan Pham","Julia Ngo"]
    emails = [f"{n.split()[0].lower()}.{n.split()[1].lower()}@gmail.com" for n in names]
    addrs = [
        "23 Elm St, Boston, MA","19 Pine Ave, San Jose, CA","14 High St, London, UK",
        "55 Nguyen Hue, Ho Chi Minh","78 Oak St, Chicago, IL","10 Broadway, New York, NY",
        "8 Rue Lafayette, Paris, FR","21 Königstr, Berlin, DE","44 Maple Dr, Los Angeles, CA","22 Bay St, Toronto, CA"
    ]
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "customer_name": np.random.choice(names, n),
        "email": np.random.choice(emails, n),
        "phone": [f"+1-202-555-{1000+i:04d}" for i in range(n)],
        "address": np.random.choice(addrs, n),
        "national_id": rng.integers(10_000_000, 99_999_999, n),
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["house","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))
    return dedupe_columns(df)

def generate_anon_synthetic(n: int) -> pd.DataFrame:
    rng = np.random.default_rng(42)
    df = pd.DataFrame({
        "application_id": [f"APP_{i:04d}" for i in range(1, n + 1)],
        "age": rng.integers(21, 65, n),
        "income": rng.integers(25_000, 150_000, n),
        "employment_length": rng.integers(0, 30, n),
        "loan_amount": rng.integers(5_000, 100_000, n),
        "loan_duration_months": rng.choice([12, 24, 36, 48, 60, 72], n),
        "collateral_value": rng.integers(8_000, 200_000, n),
        "collateral_type": rng.choice(["house","car","land","deposit"], n),
        "co_loaners": rng.choice([0,1,2], n, p=[0.7, 0.25, 0.05]),
        "credit_score": rng.integers(300, 850, n),
        "existing_debt": rng.integers(0, 50_000, n),
        "assets_owned": rng.integers(10_000, 300_000, n),
    })
    eps = 1e-9
    df["DTI"] = df["existing_debt"] / (df["income"] + eps)
    df["LTV"] = df["loan_amount"] / (df["collateral_value"] + eps)
    df["CCR"] = df["collateral_value"] / (df["loan_amount"] + eps)
    df["ITI"] = (df["loan_amount"] / (df["loan_duration_months"] + eps)) / (df["income"] + eps)
    df["CWI"] = ((1 - df["DTI"]).clip(0, 1)) * ((1 - df["LTV"]).clip(0, 1)) * (df["CCR"].clip(0, 3))
    return dedupe_columns(df)

# ─────────────────────────────────────────────
# MODEL/HARDWARE CATALOG (UI only – passed to API as hints)
LLM_MODELS = [
    # label, value (sent), hw_hint
    ("Phi-3 Mini (3.8B) — CPU OK", "phi3:3.8b", "CPU 8GB RAM (fast)"),
    ("Mistral 7B Instruct — CPU slow / GPU OK", "mistral:7b-instruct", "CPU 16GB (slow) or GPU ≥8GB"),
    ("Gemma-2 7B — CPU slow / GPU OK", "gemma2:7b", "CPU 16GB (slow) or GPU ≥8GB"),
    ("LLaMA-3 8B — GPU recommended", "llama3:8b-instruct", "GPU ≥12GB (CPU very slow)"),
    ("Qwen2 7B — GPU recommended", "qwen2:7b-instruct", "GPU ≥12GB (CPU very slow)"),
    ("Mixtral 8x7B — GPU only (big)", "mixtral:8x7b-instruct", "GPU 24–48GB"),
]
LLM_LABELS = [label for (label, _, _) in LLM_MODELS]
LLM_VALUE_BY_LABEL = {label: value for (label, value, _) in LLM_MODELS}
LLM_HINT_BY_LABEL  = {label: hint  for (label, _, hint) in LLM_MODELS}

OPENSTACK_FLAVORS = {
    "m4.medium":  "4 vCPU / 8 GB RAM — CPU-only small",
    "m8.large":   "8 vCPU / 16 GB RAM — CPU-only medium",
    "g1.a10.1":   "8 vCPU / 32 GB RAM + 1×A10 24GB",
    "g1.l40.1":   "16 vCPU / 64 GB RAM + 1×L40 48GB",
    "g2.a100.1":  "24 vCPU / 128 GB RAM + 1×A100 80GB",
}

# ─────────────────────────────────────────────
# 🏦 TAB 1 — Synthetic Data Generator
with tab_gen:
    st.subheader("🏦 Synthetic Credit Data Generator")

    rows = st.slider("Number of rows to generate", 50, 1000, 200, step=50)
    colA, colB = st.columns(2)

    with colA:
        if st.button("🔴 Generate RAW Synthetic Data (with PII)", use_container_width=True):
            raw_df = append_user_info(generate_raw_synthetic(rows))
            st.session_state.synthetic_raw_df = raw_df
            raw_path = save_to_runs(raw_df, "synthetic_raw")
            st.success(f"Generated RAW (PII) dataset with {rows} rows. Saved to {raw_path}")
            st.dataframe(raw_df.head(10), use_container_width=True)
            st.download_button(
                "⬇️ Download RAW CSV",
                raw_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(raw_path),
                "text/csv",
            )

    with colB:
        if st.button("🟢 Generate ANON Synthetic Data (ready for agent)", use_container_width=True):
            anon_df = append_user_info(generate_anon_synthetic(rows))
            st.session_state.synthetic_df = anon_df
            anon_path = save_to_runs(anon_df, "synthetic_anon")
            st.success(f"Generated ANON dataset with {rows} rows. Saved to {anon_path}")
            st.dataframe(anon_df.head(10), use_container_width=True)
            st.download_button(
                "⬇️ Download ANON CSV",
                anon_df.to_csv(index=False).encode("utf-8"),
                os.path.basename(anon_path),
                "text/csv",
            )

# ─────────────────────────────────────────────
# 🧹 TAB 2 — Anonymize & Sanitize Data
with tab_clean:
    st.subheader("🧹 Upload & Anonymize Customer Data (PII columns will be DROPPED)")
    st.markdown("Upload your **real CSV**. We drop PII columns and scrub emails/phones in text fields.")

    uploaded = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded:
        try:
            df = pd.read_csv(uploaded)
        except Exception as e:
            st.error(f"Could not read CSV: {e}")
            st.stop()

        st.write("📊 Original Data Preview:")
        st.dataframe(dedupe_columns(df.head(5)), use_container_width=True)

        sanitized, dropped_cols = drop_pii_columns(df)
        sanitized = append_user_info(sanitized)
        sanitized = dedupe_columns(sanitized)
        st.session_state.anonymized_df = sanitized

        st.success(f"Dropped PII columns: {sorted(dropped_cols) if dropped_cols else 'None'}")
        st.write("✅ Sanitized Data Preview:")
        st.dataframe(sanitized.head(5), use_container_width=True)

        fpath = save_to_runs(sanitized, "anonymized")
        st.success(f"Saved anonymized file: {fpath}")
        st.download_button(
            "⬇️ Download Clean Data",
            sanitized.to_csv(index=False).encode("utf-8"),
            os.path.basename(fpath),
            "text/csv",
        )
    else:
        st.info("Choose a CSV to see the sanitize flow.", icon="ℹ️")

# ─────────────────────────────────────────────
# 🤖 TAB 3 — Credit Appraisal by AI Assistant
with tab_run:
    st.subheader("🤖 Credit Appraisal by AI Assistant")

    # Production model banner (optional)
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.status_code == 200:
            meta = resp.json()
            if meta.get("has_production"):
                ver = (meta.get("meta") or {}).get("version", "unknown")
                src = (meta.get("meta") or {}).get("source", "production")
                st.success(f"🟢 Production model active — version: {ver} • source: {src}")
            else:
                st.warning("⚠️ No production model promoted yet — using ephemeral baseline.")
        else:
            st.info("ℹ️ Could not fetch production model meta.")
    except Exception:
        st.info("ℹ️ Production meta unavailable.")

    # 1) Model + Hardware selection (UI hints)
    with st.expander("🧠 Local LLM & Hardware Profile", expanded=True):
        c1, c2 = st.columns([1.2, 1])
        with c1:
            model_label = st.selectbox(
                "Local LLM (used for narratives/explanations)",
                LLM_LABELS,
                index=0
            )
            llm_value = LLM_VALUE_BY_LABEL[model_label]
            st.caption(f"Hint: {LLM_HINT_BY_LABEL[model_label]}")
        with c2:
            flavor = st.selectbox("OpenStack flavor / host profile", list(OPENSTACK_FLAVORS.keys()), index=0)
            st.caption(OPENSTACK_FLAVORS[flavor])
        st.caption("These are passed to the API as hints; your API can choose Ollama/Flowise backends accordingly.")

    # 2) Data Source (no sample dataset option)
    data_choice = st.selectbox(
        "Select Data Source",
        [
            "Use synthetic (ANON)",
            "Use synthetic (RAW – auto-sanitize)",
            "Use anonymized dataset",
            "Upload manually",
        ]
    )
    use_llm = st.checkbox("Use LLM narrative", value=False)
    agent_name = "credit_appraisal"

    if data_choice == "Upload manually":
        up = st.file_uploader("Upload your CSV", type=["csv"], key="manual_upload_run_file")
        if up is not None:
            st.session_state["manual_upload_name"] = up.name
            st.session_state["manual_upload_bytes"] = up.getvalue()
            st.success(f"File staged: {up.name} ({len(st.session_state['manual_upload_bytes'])} bytes)")

    # 3) Rules (with persistence)
    st.markdown("### ⚙️ Decision Rule Set")
    profile = load_metrics_profile()
    rule_mode = st.radio(
        "Choose rule mode",
        ["Classic (bank-style metrics)", "NDI (Net Disposable Income) — simple"],
        index=0,
        help="NDI = income - all monthly obligations. Approve if NDI and NDI ratio pass thresholds."
    )

    CLASSIC_DEFAULTS = {
        "max_dti": 0.45, "min_emp_years": 2, "min_credit_hist": 3, "salary_floor": 3000,
        "max_delinquencies": 2, "max_current_loans": 3, "req_min": 1000, "req_max": 200000,
        "loan_terms": [12, 24, 36, 48, 60], "threshold": 0.45, "target_rate": None, "random_band": True,
        "min_income_debt_ratio": 0.35, "compounded_debt_factor": 1.0, "monthly_debt_relief": 0.50,
    }
    NDI_DEFAULTS = {"ndi_value": 800.0, "ndi_ratio": 0.50, "threshold": 0.45, "target_rate": None, "random_band": True}

    if "classic_rules" not in st.session_state:
        st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    if "ndi_rules" not in st.session_state:
        st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    def reset_classic(): st.session_state.classic_rules = CLASSIC_DEFAULTS.copy()
    def reset_ndi():     st.session_state.ndi_rules = NDI_DEFAULTS.copy()

    if rule_mode.startswith("Classic"):
        with st.expander("Classic Metrics (with Reset & Save)", expanded=True):
            rc = st.session_state.classic_rules
            r1, r2, r3 = st.columns(3)
            with r1:
                rc["max_dti"] = st.slider("Max Debt-to-Income (DTI)", 0.0, 1.0, float(profile.get("max_dti", rc["max_dti"])), 0.01)
                rc["min_emp_years"] = st.number_input("Min Employment Years", 0, 40, int(profile.get("min_emp_years", rc["min_emp_years"])))
                rc["min_credit_hist"] = st.number_input("Min Credit History (years)", 0, 40, int(profile.get("min_credit_hist", rc["min_credit_hist"])))
            with r2:
                rc["salary_floor"] = st.number_input("Minimum Monthly Salary ($)", 500, 20000, int(profile.get("salary_floor", rc["salary_floor"])), step=500)
                rc["max_delinquencies"] = st.number_input("Max Delinquencies", 0, 10, int(profile.get("max_delinquencies", rc["max_delinquencies"])))
                rc["max_current_loans"] = st.number_input("Max Current Loans", 0, 10, int(profile.get("max_current_loans", rc["max_current_loans"])))
            with r3:
                rc["req_min"] = st.number_input("Requested Amount Min ($)", 0, 1_000_000, int(profile.get("req_min", rc["req_min"])), step=1000)
                rc["req_max"] = st.number_input("Requested Amount Max ($)", 0, 1_000_000, int(profile.get("req_max", rc["req_max"])), step=1000)
                rc["loan_terms"] = st.multiselect("Allowed Loan Terms (months)", [12,24,36,48,60,72], default=profile.get("loan_terms", rc["loan_terms"]))

            st.markdown("#### 🧮 Debt Pressure Controls")
            d1, d2, d3 = st.columns(3)
            with d1:
                rc["min_income_debt_ratio"] = st.slider("Min Income / (Compounded Debt) Ratio", 0.10, 2.00, float(profile.get("min_income_debt_ratio", rc["min_income_debt_ratio"])), 0.01)
            with d2:
                rc["compounded_debt_factor"] = st.slider("Compounded Debt Factor (× requested)", 0.5, 3.0, float(profile.get("compounded_debt_factor", rc["compounded_debt_factor"])), 0.1)
            with d3:
                rc["monthly_debt_relief"] = st.slider("Monthly Debt Relief Factor", 0.10, 1.00, float(profile.get("monthly_debt_relief", rc["monthly_debt_relief"])), 0.05)

            st.markdown("---")
            c1, c2, c3, c4 = st.columns([1,1,1,1])
            with c1:
                use_target = st.toggle("🎯 Use target approval rate", value=(profile.get("target_rate", rc["target_rate"]) is not None))
            with c2:
                rc["random_band"] = st.toggle("🎲 Randomize approval band (20–60%) when no target", value=bool(profile.get("random_band", rc["random_band"])))
            with c3:
                if st.button("↩️ Reset to defaults"):
                    reset_classic()
                    st.rerun()
            with c4:
                if st.button("💾 Save Metrics (Classic)"):
                    to_save = {
                        "mode": "classic",
                        "max_dti": rc["max_dti"],
                        "min_emp_years": rc["min_emp_years"],
                        "min_credit_hist": rc["min_credit_hist"],
                        "salary_floor": rc["salary_floor"],
                        "max_delinquencies": rc["max_delinquencies"],
                        "max_current_loans": rc["max_current_loans"],
                        "req_min": rc["req_min"],
                        "req_max": rc["req_max"],
                        "loan_terms": rc["loan_terms"],
                        "min_income_debt_ratio": rc["min_income_debt_ratio"],
                        "compounded_debt_factor": rc["compounded_debt_factor"],
                        "monthly_debt_relief": rc["monthly_debt_relief"],
                        "threshold": None if use_target else rc["threshold"],
                        "target_rate": (profile.get("target_rate") if use_target else None) or rc.get("target_rate"),
                        "random_band": rc["random_band"],
                    }
                    save_metrics_profile(to_save)
                    st.success("Saved Classic metrics profile.")

            if use_target:
                rc["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, float(profile.get("target_rate", rc.get("target_rate") or 0.40)), 0.01)
                rc["threshold"] = None
            else:
                rc["threshold"] = st.slider("Model score threshold", 0.0, 1.0, float(profile.get("threshold", rc["threshold"])), 0.01)
                rc["target_rate"] = None

    else:
        with st.expander("NDI Metrics (with Reset & Save)", expanded=True):
            rn = st.session_state.ndi_rules
            n1, n2 = st.columns(2)
            with n1:
                rn["ndi_value"] = st.number_input("Min NDI (Net Disposable Income) $/month", 0.0, 10000.0, float(profile.get("ndi_value", rn["ndi_value"])), step=50.0)
            with n2:
                rn["ndi_ratio"] = st.slider("Min NDI / Income ratio", 0.0, 1.0, float(profile.get("ndi_ratio", rn["ndi_ratio"])), 0.01)
            st.caption("NDI = income - all monthly obligations (rent, food, loans, cards, etc.).")

            st.markdown("---")
            c1, c2, c3, c4 = st.columns([1,1,1,1])
            with c1:
                use_target = st.toggle("🎯 Use target approval rate", value=(profile.get("target_rate", rn["target_rate"]) is not None))
            with c2:
                rn["random_band"] = st.toggle("🎲 Randomize approval band (20–60%) when no target", value=bool(profile.get("random_band", rn["random_band"])))
            with c3:
                if st.button("↩️ Reset to defaults (NDI)"):
                    reset_ndi()
                    st.rerun()
            with c4:
                if st.button("💾 Save Metrics (NDI)"):
                    to_save = {
                        "mode": "ndi",
                        "ndi_value": rn["ndi_value"],
                        "ndi_ratio": rn["ndi_ratio"],
                        "threshold": None if use_target else rn["threshold"],
                        "target_rate": (profile.get("target_rate") if use_target else None) or rn.get("target_rate"),
                        "random_band": rn["random_band"],
                    }
                    save_metrics_profile(to_save)
                    st.success("Saved NDI metrics profile.")

            if use_target:
                rn["target_rate"] = st.slider("Target approval rate", 0.05, 0.95, float(profile.get("target_rate", rn.get("target_rate") or 0.40)), 0.01)
                rn["threshold"] = None
            else:
                rn["threshold"] = st.slider("Model score threshold", 0.0, 1.0, float(profile.get("threshold", rn["threshold"])), 0.01)
                rn["target_rate"] = None

    # 4) Run
    if st.button("🚀 Run Agent", use_container_width=True):
        try:
            files = None
            data: Dict[str, Any] = {
                "use_llm_narrative": str(use_llm).lower(),
                "llm_model": llm_value,
                "hardware_flavor": flavor,
            }
            if rule_mode.startswith("Classic"):
                rc = st.session_state.classic_rules
                data.update({
                    "min_employment_years": str(rc["min_emp_years"]),
                    "max_debt_to_income": str(rc["max_dti"]),
                    "min_credit_history_length": str(rc["min_credit_hist"]),
                    "max_num_delinquencies": str(rc["max_delinquencies"]),
                    "max_current_loans": str(rc["max_current_loans"]),
                    "requested_amount_min": str(rc["req_min"]),
                    "requested_amount_max": str(rc["req_max"]),
                    "loan_term_months_allowed": ",".join(map(str, rc["loan_terms"])) if rc["loan_terms"] else "",
                    "min_income_debt_ratio": str(rc["min_income_debt_ratio"]),
                    "compounded_debt_factor": str(rc["compounded_debt_factor"]),
                    "monthly_debt_relief": str(rc["monthly_debt_relief"]),
                    "salary_floor": str(rc["salary_floor"]),
                    "threshold": "" if rc["threshold"] is None else str(rc["threshold"]),
                    "target_approval_rate": "" if rc["target_rate"] is None else str(rc["target_rate"]),
                    "random_band": str(rc["random_band"]).lower(),
                    "random_approval_band": str(rc["random_band"]).lower(),
                    "rule_mode": "classic",
                })
            else:
                rn = st.session_state.ndi_rules
                data.update({
                    "ndi_value": str(rn["ndi_value"]),
                    "ndi_ratio": str(rn["ndi_ratio"]),
                    "threshold": "" if rn["threshold"] is None else str(rn["threshold"]),
                    "target_approval_rate": "" if rn["target_rate"] is None else str(rn["target_rate"]),
                    "random_band": str(rn["random_band"]).lower(),
                    "random_approval_band": str(rn["random_band"]).lower(),
                    "rule_mode": "ndi",
                })

            def prep_and_pack(df: pd.DataFrame, filename: str):
                safe = dedupe_columns(df)
                safe, _ = drop_pii_columns(safe)
                safe = strip_policy_banned(safe)
                safe = to_agent_schema(safe)
                buf = io.StringIO()
                safe.to_csv(buf, index=False)
                return {"file": (filename, buf.getvalue().encode("utf-8"), "text/csv")}

            if data_choice == "Use synthetic (ANON)":
                if "synthetic_df" not in st.session_state:
                    st.warning("No ANON synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_df, "synthetic_anon.csv")

            elif data_choice == "Use synthetic (RAW – auto-sanitize)":
                if "synthetic_raw_df" not in st.session_state:
                    st.warning("No RAW synthetic dataset found. Generate it in the first tab."); st.stop()
                files = prep_and_pack(st.session_state.synthetic_raw_df, "synthetic_raw_sanitized.csv")

            elif data_choice == "Use anonymized dataset":
                if "anonymized_df" not in st.session_state:
                    st.warning("No anonymized dataset found. Create it in the second tab."); st.stop()
                files = prep_and_pack(st.session_state.anonymized_df, "anonymized.csv")

            elif data_choice == "Upload manually":
                up_name = st.session_state.get("manual_upload_name")
                up_bytes = st.session_state.get("manual_upload_bytes")
                if not up_name or not up_bytes:
                    st.warning("Please upload a CSV first."); st.stop()
                try:
                    tmp_df = pd.read_csv(io.BytesIO(up_bytes))
                    files = prep_and_pack(tmp_df, up_name)
                except Exception:
                    files = {"file": (up_name, up_bytes, "text/csv")}
            else:
                st.error("Unknown data source selection."); st.stop()

            r = requests.post(f"{API_URL}/v1/agents/{agent_name}/run", data=data, files=files, timeout=180)
            if r.status_code != 200:
                st.error(f"Run failed ({r.status_code}): {r.text}"); st.stop()

            res = r.json()
            st.session_state.last_run_id = res.get("run_id")
            result = res.get("result", {}) or {}
            summary = result.get("summary", {}) or {}
            st.success(f"✅ Run succeeded! Run ID: {st.session_state.last_run_id}")

            # Pull merged.csv for dashboards/review
            rid = st.session_state.last_run_id
            merged_url = f"{API_URL}/v1/runs/{rid}/report?format=csv"
            merged_bytes = requests.get(merged_url, timeout=30).content
            merged_df = pd.read_csv(io.BytesIO(merged_bytes))
            st.session_state["last_merged_df"] = merged_df

            # Persist AI output as a reviewable CSV for later Human Review
            ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            ai_out_path = os.path.join(RUNS_DIR, f"ai-appraisal-outputs-{ts}.csv")
            merged_df.to_csv(ai_out_path, index=False)
            st.info(f"Saved AI output for review: {ai_out_path}")

            # Decision filter
            st.markdown("#### Filter by Decision")
            decision_filter = st.radio("Show:", ["All", "approved", "denied"], horizontal=True)
            df_view = merged_df.copy()
            if decision_filter != "All" and "decision" in df_view.columns:
                df_view = df_view[df_view["decision"] == decision_filter]

            # Per-row metrics met/not met
            st.markdown("#### Why each decision?")
            if "rule_reasons" in df_view.columns:
                rr = df_view["rule_reasons"].apply(try_json)
                df_view["metrics_met"] = rr.apply(lambda d: ", ".join(sorted([k for k,v in (d or {}).items() if v is True])) if isinstance(d, dict) else "")
                df_view["metrics_unmet"] = rr.apply(lambda d: ", ".join(sorted([k for k,v in (d or {}).items() if v is False])) if isinstance(d, dict) else "")
            cols_show = [c for c in [
                "application_id","decision","score","loan_amount","income",
                "metrics_met","metrics_unmet",
                "proposed_loan_option","proposed_consolidation_loan",
                "top_feature","explanation"
            ] if c in df_view.columns]
            st.dataframe(df_view[cols_show].head(50), use_container_width=True)

            # Buyback / consolidation for denied
            st.markdown("#### Proposed Buyback / Consolidation (for denied)")
            if "decision" in merged_df.columns:
                denied_df = merged_df[merged_df["decision"] == "denied"].copy()
                if "proposed_consolidation_loan" in denied_df.columns and not denied_df.empty:
                    st.dataframe(denied_df[["application_id","proposed_consolidation_loan"]].head(25), use_container_width=True)
                else:
                    st.info("No consolidation proposals present in results.")

            # Dashboard
            st.markdown("### 📊 Dashboard")
            # 1) Pie chart Approved/Denied
            if "decision" in merged_df.columns:
                pie_df = merged_df["decision"].value_counts().rename_axis("Decision").reset_index(name="Count")
                chart1 = alt.Chart(pie_df).mark_arc().encode(
                    theta="Count:Q",
                    color="Decision:N",
                    tooltip=["Decision","Count"]
                ).properties(height=250)
                st.altair_chart(chart1, use_container_width=True)

            # 2) Top accepted loan values (descending)
            if set(["decision","loan_amount","application_id"]).issubset(set(merged_df.columns)):
                acc_df = merged_df[merged_df["decision"]=="approved"].copy()
                if not acc_df.empty:
                    top_acc = acc_df.sort_values("loan_amount", ascending=False).head(25)
                    chart2 = alt.Chart(top_acc).mark_bar().encode(
                        x=alt.X("application_id:N", sort="-y", title="Application"),
                        y=alt.Y("loan_amount:Q", title="Loan Amount"),
                        tooltip=["application_id","loan_amount"]
                    ).properties(height=280)
                    st.altair_chart(chart2, use_container_width=True)

            # 3) Top 10 proposed loan plans
            if "proposed_loan_option" in merged_df.columns:
                p = merged_df["proposed_loan_option"].dropna().astype(str)
                if len(p) > 0:
                    top_plans = p.value_counts().head(10).rename_axis("plan").reset_index(name="count")
                    chart3 = alt.Chart(top_plans).mark_bar().encode(
                        x=alt.X("plan:N", sort="-y", title="Plan"),
                        y=alt.Y("count:Q", title="Count"),
                        tooltip=["plan","count"]
                    ).properties(height=280)
                    st.altair_chart(chart3, use_container_width=True)

            # 4) Average salary vs loan & proposed loan
            num_cols = [c for c in ["income","loan_amount"] if c in merged_df.columns]
            if len(num_cols) >= 1:
                group = merged_df.copy()
                if "proposed_loan_option" not in group.columns:
                    group["proposed_loan_option"] = "N/A"
                agg = group.groupby("proposed_loan_option").agg(
                    avg_income=("income","mean"),
                    avg_loan_amount=("loan_amount","mean"),
                    count=("application_id","count")
                ).reset_index().sort_values("avg_loan_amount", ascending=False).head(15)
                chart4 = alt.Chart(agg).mark_bar().encode(
                    x=alt.X("proposed_loan_option:N", sort="-y", title="Proposed Plan"),
                    y=alt.Y("avg_loan_amount:Q", title="Avg Loan Amount"),
                    tooltip=["proposed_loan_option","avg_income","avg_loan_amount","count"]
                ).properties(height=280)
                st.altair_chart(chart4, use_container_width=True)

            # Downloads
            cdl1, cdl2, cdl3, cdl4, cdl5 = st.columns(5)
            with cdl1: st.markdown(f"[⬇️ PDF report]({API_URL}/v1/runs/{rid}/report?format=pdf)")
            with cdl2: st.markdown(f"[⬇️ Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
            with cdl3: st.markdown(f"[⬇️ Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
            with cdl4: st.markdown(f"[⬇️ Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
            with cdl5: st.markdown(f"[⬇️ JSON]({API_URL}/v1/runs/{rid}/report?format=json)")

        except Exception as e:
            st.exception(e)

    # Re-download quick section
    if st.session_state.get("last_run_id"):
        st.markdown("---")
        st.subheader("📥 Download Latest Outputs")
        rid = st.session_state.last_run_id
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1: st.markdown(f"[⬇️ PDF]({API_URL}/v1/runs/{rid}/report?format=pdf)")
        with col2: st.markdown(f"[⬇️ Scores CSV]({API_URL}/v1/runs/{rid}/report?format=scores_csv)")
        with col3: st.markdown(f"[⬇️ Explanations CSV]({API_URL}/v1/runs/{rid}/report?format=explanations_csv)")
        with col4: st.markdown(f"[⬇️ Merged CSV]({API_URL}/v1/runs/{rid}/report?format=csv)")
        with col5: st.markdown(f"[⬇️ JSON]({API_URL}/v1/runs/{rid}/report?format=json)")

# ─────────────────────────────────────────────
# 🧑‍⚖️ TAB 4 — Human Review
with tab_review:
    st.subheader("🧑‍⚖️ Human Review — Correct AI Decisions & Score Agreement")

    ai_outputs = sorted(glob(os.path.join(RUNS_DIR, "ai-appraisal-outputs-*.csv")))
    if not ai_outputs:
        st.info("No saved AI outputs yet. Run an appraisal first.")
        st.stop()

    choice = st.selectbox(
        "Choose an AI output to review",
        ai_outputs,
        index=len(ai_outputs)-1 if ai_outputs else 0,
        format_func=os.path.basename
    )
    dfm = pd.read_csv(choice)

    st.markdown("#### 1) Select rows to review and correct")
    edit_cols = []
    if "decision" in dfm.columns: edit_cols.append("decision")
    if "rule_reasons" in dfm.columns: edit_cols.append("rule_reasons")
    editable = dfm[["application_id"] + edit_cols].copy()
    editable.rename(columns={"decision": "ai_decision"}, inplace=True)
    editable["human_decision"] = editable["ai_decision"]
    editable["human_rule_reasons"] = editable.get("rule_reasons", "")

    edited = st.data_editor(
        editable,
        num_rows="dynamic",
        use_container_width=True,
        key="review_editor",
        column_config={
            "human_decision": st.column_config.SelectboxColumn(options=["approved","denied"]),
        }
    )

    st.markdown("#### 2) Compute agreement score")
    if st.button("Compute agreement score"):
        if {"ai_decision","human_decision"}.issubset(edited.columns):
            score = agreement_score(edited["ai_decision"], edited["human_decision"])
            st.success(f"Agreement score (AI vs human): {score:.2f}%")
            st.session_state["last_agreement_score"] = score
        else:
            st.warning("Missing decision columns to compute score.")

    st.markdown("#### 3) Export reviewed CSV & Submit feedback to API")
    review_ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    review_path = os.path.join(RUNS_DIR, f"human-reviewed-{review_ts}.csv")
    csv_bytes = edited.to_csv(index=False).encode("utf-8")
    st.download_button("⬇️ Download reviewed CSV", csv_bytes, file_name=os.path.basename(review_path))
    if st.button("Submit feedback"):
        try:
            payload = []
            for _, row in edited.iterrows():
                payload.append({
                    "application_id": row["application_id"],
                    "ai_decision": row.get("ai_decision"),
                    "human_decision": row.get("human_decision"),
                    "human_rule_reasons": row.get("human_rule_reasons"),
                })
            r = requests.post(f"{API_URL}/v1/training/feedback", json={"items": payload}, timeout=20)
            if r.ok:
                st.success(r.json())
                # persist reviewed CSV on success
                with open(review_path, "wb") as f:
                    f.write(csv_bytes)
                st.info(f"Saved reviewed CSV: {review_path}")
            else:
                st.error(r.text)
        except Exception as e:
            st.error(f"Feedback failed: {e}")

# ─────────────────────────────────────────────
# 🔁 TAB 5 — Training (Feedback → Retrain)
with tab_train:
    st.subheader("🔁 Human Feedback → Retrain (simple workflow)")

    # pick last reviewed CSV (if any)
    reviewed_list = sorted(glob(os.path.join(RUNS_DIR, "human-reviewed-*.csv")))
    last_review = reviewed_list[-1] if reviewed_list else None

    st.markdown("1) Upload/submit feedback in 'Human Review'; 2) Retrain; 3) Promote.")
    st.markdown("#### Retrain Config")
    cfg = {
        "reviewed_csv": last_review,
        "base_csv_globs": [os.path.join(RUNS_DIR, "ai-appraisal-outputs-*.csv")],
        "cutoff_date": "2024-01-01",
    }
    st.code(json.dumps(cfg, indent=2), language="json")

    colA, colB = st.columns([1,1])
    with colA:
        if st.button("🚀 Train candidate model"):
            try:
                r = requests.post(f"{API_URL}/v1/training/train", json=cfg, timeout=60)
                if r.ok:
                    st.success(r.json())
                    st.session_state["last_train_job"] = r.json().get("job_id")
                else:
                    st.error(r.text)
            except Exception as e:
                st.error(f"Train failed: {e}")
    with colB:
        if st.button("⬆️ Promote last candidate to PRODUCTION"):
            try:
                r = requests.post(f"{API_URL}/v1/training/promote", timeout=30)
                st.write(r.json() if r.ok else r.text)
            except Exception as e:
                st.error(f"Promote failed: {e}")

    st.markdown("---")
    st.markdown("#### Production Model")
    try:
        resp = requests.get(f"{API_URL}/v1/training/production_meta", timeout=5)
        if resp.ok:
            st.json(resp.json())
        else:
            st.info("No production model yet.")
    except Exception as e:
        st.warning(f"Could not load production meta: {e}")
