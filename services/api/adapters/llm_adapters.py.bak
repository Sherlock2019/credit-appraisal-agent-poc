# services/api/adapters/llm_adapters.py
from __future__ import annotations

# Minimal local LLM registry used by /v1/system/llms and as a hint in /v1/agents/.../run
LLM_CATALOG = {
    "Phi-3 Mini (3.8B) — CPU OK": {
        "id": "phi3:3.8b",
        "provider": "ollama",
        "hint": "CPU 8GB RAM (fast)",
    },
    "Mistral 7B Instruct — CPU slow / GPU OK": {
        "id": "mistral:7b-instruct",
        "provider": "ollama",
        "hint": "CPU 16GB (slow) or GPU ≥8GB",
    },
    "Gemma-2 7B — CPU slow / GPU OK": {
        "id": "gemma2:7b",
        "provider": "ollama",
        "hint": "CPU 16GB (slow) or GPU ≥8GB",
    },
    "LLaMA-3 8B — GPU recommended": {
        "id": "llama3:8b-instruct",
        "provider": "ollama",
        "hint": "GPU ≥12GB (CPU very slow)",
    },
    "Qwen2 7B — GPU recommended": {
        "id": "qwen2:7b-instruct",
        "provider": "ollama",
        "hint": "GPU ≥12GB (CPU very slow)",
    },
    "Mixtral 8x7B — GPU only (big)": {
        "id": "mixtral:8x7b-instruct",
        "provider": "ollama",
        "hint": "GPU 24–48GB",
    },
}
